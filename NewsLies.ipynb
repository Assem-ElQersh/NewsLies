{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa82ec4",
   "metadata": {},
   "source": [
    "# Arabic Fake News Detection using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00919f",
   "metadata": {},
   "source": [
    "This notebook implements an LSTM model to detect Arabic fake news. The dataset is preprocessed, and various NLP techniques are applied before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3729df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e8016",
   "metadata": {},
   "source": [
    "### Step 1: Load the Arabic stopwords from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc921fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to filter out common words that don't contribute much to the meaning of the text.\n",
    "stop_words = set(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb94b94",
   "metadata": {},
   "source": [
    "### Step 2: Specify the path to your dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ae128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the data files (articles, sources) are located.\n",
    "dataset_dir = \"/kaggle/input/arabic-fake-news-dataset-afnd/AFND/Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5493f",
   "metadata": {},
   "source": [
    "### Step 3: Read sources.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains information about the credibility of each source (credible, not credible, undecided).\n",
    "sources_file_path = '/kaggle/input/arabic-fake-news-dataset-afnd/AFND/sources.json'\n",
    "with open(sources_file_path, 'r', encoding='utf-8') as sources_file:\n",
    "    sources_data = json.load(sources_file)\n",
    "\n",
    "# Convert the sources data into a DataFrame for easier processing\n",
    "sources_df = pd.DataFrame(list(sources_data.items()), columns=['source', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214e373",
   "metadata": {},
   "source": [
    "### Step 4: Read scraped_articles.json for each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the articles from each source are stored. We'll iterate over each source to load its articles.\n",
    "articles_data = []\n",
    "for source in sources_df['source']:\n",
    "    scraped_articles_path = os.path.join(dataset_dir, source, 'scraped_articles.json')\n",
    "    \n",
    "    # Check if the file exists before attempting to read it\n",
    "    if os.path.exists(scraped_articles_path):\n",
    "        with open(scraped_articles_path, 'r', encoding='utf-8') as articles_file:\n",
    "            source_articles_dict = json.load(articles_file)\n",
    "            source_articles_list = source_articles_dict.get(\"articles\", [])\n",
    "            \n",
    "            # Add a 'source' key to each article to keep track of where it came from\n",
    "            for article in source_articles_list:\n",
    "                article['source'] = source\n",
    "            \n",
    "            # Add all articles to the main list\n",
    "            articles_data.extend(source_articles_list)\n",
    "\n",
    "# Convert articles_data to a DataFrame for processing\n",
    "articles_df = pd.DataFrame(articles_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5550b3",
   "metadata": {},
   "source": [
    "### Step 5: Merge the articles with their sources(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bcddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This merges the article data with the credibility label from the sources data.\n",
    "merged_df = pd.merge(articles_df, sources_df, how='inner', left_on='source', right_on='source')\n",
    "\n",
    "# Optional: Display the first few rows and the shape of the merged DataFrame\n",
    "merged_df.head()\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a18097",
   "metadata": {},
   "source": [
    "### Step 6: Plot the distribution of article labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar('Not Credible Articles', len(merged_df[merged_df['label'] == 'not credible']), color='orange')\n",
    "plt.bar('Credible Articles', len(merged_df[merged_df['label'] == 'credible']), color='green')\n",
    "plt.bar('Undecided Articles', len(merged_df[merged_df['label'] == 'undecided']), color='gray')\n",
    "plt.title('Distribution of Articles', size=15)\n",
    "plt.xlabel('Articles Type', size=15)\n",
    "plt.ylabel('# of Articles', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e4a063",
   "metadata": {},
   "source": [
    "### Step 7: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df['text']  # Features: the text of the articles\n",
    "y = merged_df['label']  # Labels: credibility labels\n",
    "\n",
    "# Normalize text to lowercase\n",
    "# This makes the text uniform by converting all characters to lowercase.\n",
    "X = X.apply(lambda text: text.lower())\n",
    "\n",
    "# Remove Arabic stopwords\n",
    "# Stopwords are common words like \"and\", \"the\", etc., that are removed because they don't add much value.\n",
    "X = X.apply(lambda text: ' '.join([word for word in text.split() if word not in stop_words]))\n",
    "\n",
    "# Apply Stemming with ISRIStemmer\n",
    "# Stemming reduces words to their root form, which helps in reducing the vocabulary size.\n",
    "stemmer = ISRIStemmer()\n",
    "X = X.apply(lambda text: ' '.join([stemmer.stem(word) for word in text.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c376d5a",
   "metadata": {},
   "source": [
    "### Step 8: Convert labels to numerical format using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning models work better with numerical data, so we encode the labels as numbers.\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf2113",
   "metadata": {},
   "source": [
    "### Step 9: Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step splits the dataset into training data (used to train the model) and testing data (used to evaluate the model).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4782962f",
   "metadata": {},
   "source": [
    "### Step 10: Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfaac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization converts text into sequences of numbers (each number represents a word).\n",
    "# Padding ensures all sequences have the same length by adding zeros to shorter sequences.\n",
    "max_vocab = 10000  # Maximum number of words to keep in the vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "max_sequence_length = 128  # Set an appropriate value for sequence length\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d91619",
   "metadata": {},
   "source": [
    "### Step 11: Build the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182bbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM (Long Short-Term Memory) is a type of RNN (Recurrent Neural Network) suitable for sequential data like text.\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer to convert words into dense vectors of fixed size\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "\n",
    "# Add a SpatialDropout1D layer for regularization (helps prevent overfitting)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# Add LSTM layers with units (number of neurons)\n",
    "model.add(LSTM(units=64, return_sequences=True))  # First LSTM layer returns sequences\n",
    "model.add(LSTM(units=64))  # Second LSTM layer does not return sequences\n",
    "\n",
    "# Add a Dense layer with softmax activation for multi-class classification\n",
    "model.add(Dense(units=len(np.unique(y_encoded)), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854b834",
   "metadata": {},
   "source": [
    "### Step 12: Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ab464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step defines the loss function, optimizer, and evaluation metrics.\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the Model Summary\n",
    "# This shows the architecture of the model, including the number of layers and parameters.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306d2f16",
   "metadata": {},
   "source": [
    "### Step 13: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model using the training data, and monitor validation loss to prevent overfitting.\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train_padded, y_train, epochs=10, validation_split=0.2, batch_size=64, shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b073b8b",
   "metadata": {},
   "source": [
    "### Step 14: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, we evaluate the model's performance on the test data.\n",
    "y_pred_probs = model.predict(X_test_padded)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Calculate and display the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display a detailed classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d32a9",
   "metadata": {},
   "source": [
    "### Step 15: Plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix helps visualize the performance of the model, showing true vs. predicted labels.\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
